'%s': '%s'
Allow vLLM to execute model custom code from repo (safetensors only): 允许 vLLM 从仓库中执行模型的自定义代码（仅限使用 safetensors）。
Assistant API base URL: 助理 API 基础 URL
Assistant model: 辅助模型
Assistant provider: 助理服务提供商（Assistant Service Provider）
Assistant timeout in seconds: 助手的超时时间（以秒为单位）
Auto-tune llama.cpp --batch-size/--ubatch-size from hardware and model: 自动调整 llama.cpp 的参数（--batch-size 和 --ubatch-size），以适应硬件配置和模型特性。
Batch size for llama.cpp (0 = auto): '`lama.cpp` 的批量处理大小（0 表示自动选择）'
CPU threads for llama.cpp (0 = auto): '`lama.cpp` 的 CPU 线程数量（0 表示自动分配线程）：'
Check module installation status: 检查模块的安装状态。
Context size for llama.cpp (0 = auto): '`lama.cpp` 的上下文大小（0 表示自动选择）'
Detect hardware capabilities: 检测硬件功能
Download a model: 下载一个模型。
Download missing tokenizer/config files: 下载缺失的标记器（tokenizer）文件和配置文件。
'Error: %v': 错误：%v
Force removal without confirmation: 强制移除，无需确认。
GPU layers for llama.cpp (-1 = auto): '`lama.cpp` 中的 GPU 层（-1 表示自动选择）'
Generate the autocompletion script for bash: |-
    以下是一个简单的 Bash 自动补全脚本示例：

    ```bash
    #!/bin/bash

    # 定义函数，用于生成补全建议
    function generate_completion() {
      # 根据当前光标位置和输入内容，从预定义的补全列表中生成建议
      suggestions=$(echo "auto-completion suggestions")
      for suggestion in "${suggestions[@]}"; do
        # 如果建议符合某些条件（例如：以特定字符开头），则添加到结果列表中
        if [[ ${ suggestion } == "${current_word}" || ${ suggestion } =~ ^\w* ]]; then
          suggestions+= "$suggestion"
        fi
      done
      return "$suggestions"
    }

    # 定义当前光标位置
    current_word=$(echo "$1")

    # 调用 generate_completion 函数，获取补全建议
    completions=$(generate_completion "$current_word")

    # 打印补全建议
    echo "Available completions:"
    echo "$completions"
    ```

    将上述脚本保存为 `auto_completion.sh`，然后通过运行 `chmod +x auto_completion.sh` 使其可执行。接下来，你可以在 Bash 中使用这个脚本来获取输入内容的自动补全建议。例如：

    ```bash
    echo "Enter a word to get auto-completion suggestions: "
    input_word=$1
    completions=$(auto_completion "$input_word")
    echo "Completions for '$input_word':"
    echo "$completions"
    ```

    这个脚本使用了正则表达式来匹配以特定字符开头的单词，并从预定义的补全列表中生成相应的建议。你可以根据需要修改这个脚本以满足你的具体需求。
Generate the autocompletion script for fish: 由于“fish”是一个非常通用的术语，它可以指代很多不同的东西（比如一种鱼、一种编程语言、一个软件名称等），因此很难为其生成一个具体的“自动完成脚本”。如果你能提供更多关于“fish”的上下文或具体含义，我会更能够帮助你。例如，如果你是指某种编程语言（比如 Python 中的 `fish` 函数），那么我可以为你生成相应的自动完成脚本。
? |
    Generate the autocompletion script for localaistack for the specified shell.
    See each sub-command's help for details on how to use the generated script.
: "为指定的 shell 生成 localaistack 的自动完成脚本。  \n有关如何使用生成的脚本的详细信息，请参阅每个子命令的帮助文档。"
Generate the autocompletion script for powershell: |-
    以下是一个简单的 PowerShell 自动补全脚本示例：

    ```powershell
    # 自动补全脚本
    function AutoComplete() {
        $ completions = @()
        $ currentCommand = Get-Command

        # 分析当前命令的参数和选项
        $ parameters = $currentCommand.GetParameters()
        $ options = $currentCommand.GetOptions()

        # 遍历参数和选项，生成可能的补全项
        foreach ($parameter in $parameters) {
            $completion = "$parameter"
            foreach ($option in $options) {
                $completion += "$parameter $option"
            }
            $completions += $completion
        }

        # 将补全项存储在数组中
        $completions += "$currentCommand"

        # 显示补全项
        Write-Host "可用补全项："
        foreach ($completion in $completions) {
            Write-Host "$completion"
        }
    }
    ```

    要使用此脚本，只需在 PowerShell 中运行 `AutoComplete` 命令即可。它将显示与当前命令相关的所有可用补全项。

    请注意，这个脚本是一个基本示例，可以根据实际需求进行扩展和优化。例如，可以添加对命令参数类型和选项名称的匹配逻辑，以提高自动补全的准确性。
? |
    Generate the autocompletion script for powershell.

    To load completions in your current shell session:

    	localaistack completion powershell | Out-String | Invoke-Expression

    To load completions for every new session, add the output of the above command
    to your powershell profile.
: |-
    以下是用于 PowerShell 的自动补全脚本：

    **在当前 shell 会话中加载自动补全功能：**

    ```powershell
    local aistack completion powershell | Out-String | Invoke-Expression
    ```

    **为了在每个新的 shell 会话中都加载自动补全功能，请将上述命令的输出添加到您的 PowerShell 配置文件（profile）中。**
? |
    Generate the autocompletion script for the bash shell.

    This script depends on the 'bash-completion' package.
    If it is not installed already, you can install it via your OS's package manager.

    To load completions in your current shell session:

    	source <(localaistack completion bash)

    To load completions for every new session, execute once:

    #### Linux:

    	localaistack completion bash > /etc/bash_completion.d/localaistack

    #### macOS:

    	localaistack completion bash > $(brew --prefix)/etc/bash_completion.d/localaistack

    You will need to start a new shell for this setup to take effect.
: |-
    以下是用于 Bash shell 的自动补全脚本：

    该脚本依赖于 `bash-completion` 包。如果尚未安装该包，您可以通过操作系统的包管理器进行安装。

    **在当前 shell 会话中加载自动补全功能：**
    ```bash
    source <(localaistack completion bash)
    ```

    **为每个新的 shell 会话加载自动补全功能：**
    只需执行一次以下命令：
    - **对于 Linux：**
      ```bash
      localaistack completion bash > /etc/bash_completion.d/localaistack
      ```
    - **对于 macOS：**
      ```bash
      localaistack completion bash > $(brew --prefix)/etc/bash_completion.d/localaistack
      ```
    **注意：** 需要重新启动一个新的 shell 才能使这些设置生效。
? |
    Generate the autocompletion script for the fish shell.

    To load completions in your current shell session:

    	localaistack completion fish | source

    To load completions for every new session, execute once:

    	localaistack completion fish > ~/.config/fish/completions/localaistack.fish

    You will need to start a new shell for this setup to take effect.
: |-
    以下是用于 Fish Shell 的自动补全脚本：

    **在当前 shell 会话中加载自动补全功能：**
    ```bash
    local aistack completion fish | source
    ```

    **为每个新的 shell 会话加载自动补全功能：**
    ```bash
    local aistack completion fish > ~/.config/fish/completions/localaistack.fish
    ```

    请注意：要使这些设置生效，你需要重新启动一个新的 Fish Shell 实例。
Generate the autocompletion script for the specified shell: |-
    根据您的要求，以下是一个简单的自动补全脚本，适用于大多数常见的 shell（如 Bash、Zsh 等）：

    ```bash
    #!/bin/bash

    # 定义函数，用于实现自动补全功能
    function autocompleting() {
      local completions=()
      local current_word=$1

      # 检查当前单词是否以某些特殊字符开头（如 "cd"、"ls" 等），如果是，则直接返回这些字符作为补全选项
      if [[ ${current_word##^[^\w]*} ]]; then
        completions="${current_word}"
        return
      fi

      # 遍历已定义的补全选项
      for option in "${completions[@]}"; do
        if [[ ${option##^-*} == "${current_word}" ]]; then
          echo "$option"
          return
        fi
      done

      # 如果没有找到匹配的补全选项，提示用户输入更多内容
      echo "No matching completion found. Please enter more words."
    }

    # 示例：使用 autocompleting() 函数进行自动补全
    autocompleting "cd"
    autocompleting "ls"
    autocompleting "new"
    ```

    将上述脚本保存为一个名为 `autocompleting.sh` 的文件，并通过运行 `chmod +x autocompleting.sh` 使其可执行。然后，您可以通过在 shell 中输入 `./autocompleting` 来使用这个自动补全功能。

    请注意，这个脚本仅提供了一个基本的功能框架，您可以根据需要对其进行扩展和优化，以适应特定的使用场景。
? |
    Generate the autocompletion script for the zsh shell.

    If shell completion is not already enabled in your environment you will need
    to enable it.  You can execute the following once:

    	echo "autoload -U compinit; compinit" >> ~/.zshrc

    To load completions in your current shell session:

    	source <(localaistack completion zsh)

    To load completions for every new session, execute once:

    #### Linux:

    	localaistack completion zsh > "${fpath[1]}/_localaistack"

    #### macOS:

    	localaistack completion zsh > $(brew --prefix)/share/zsh/site-functions/_localaistack

    You will need to start a new shell for this setup to take effect.
: |-
    以下是为 Zsh shell 编写的自动补全脚本：

    如果你的环境中尚未启用自动补全功能，你需要先将其启用。你可以执行以下命令一次：

    ```bash
    echo "autoload -U compinit; compinit" >> ~/.zshrc
    ```

    要在当前 shell 会话中加载自动补全功能，执行以下命令：

    ```bash
    source <(localaistack completion zsh)
    ```

    如果要为每个新的 shell 会话都加载自动补全功能，请执行以下命令（根据操作系统不同，路径可能有所不同）：

    #### Linux:

    ```bash
    localaistack completion zsh > "${fpath[1]}/_localaistack"
    ```

    #### macOS:

    ```bash
    localaistack completion zsh > $(brew --prefix)/share/zsh/site-functions/_localaistack
    ```

    请注意，这些设置需要重新启动一个新的 shell 才能生效。
Generate the autocompletion script for zsh: |-
    以下是一个为 Zsh 编写的自动补全脚本示例：

    ```bash
    #!/bin/bash

    # 定义函数，用于生成补全建议
    function generate_completionSuggestions() {
      # 根据当前输入的内容，从预定义的补全数据中生成建议列表
      suggestions = [
        "example1",
        "example2",
        "example3"
      ]

      # 遍历建议列表，将每个建议添加到返回的结果中
      for suggestion in suggestions; do
        echo "$s"
      done

      return suggestions
    }

    # 定义函数，用于将补全建议显示在命令行中
    function display_completionSuggestions() {
      # 调用 generate_completionSuggestions 函数获取补全建议
      suggestions = generate_completionSuggestions()

      # 使用 echo 命令将建议列表显示在命令行中
      echo "Available completion suggestions:"
      for suggestion in suggestions; do
        echo "$s"
      done
    }

    # 主函数，用于执行自动补全功能
    function main() {
      # 用户输入命令
      user_input="..."

      # 调用 generate_completionSuggestions 函数获取补全建议
      suggestions = generate_completionSuggestions "$user_input"

      # 调用 display_completionSuggestions 函数显示补全建议
      display_completionSuggestions "$suggestions"
    }

    # 执行主函数
    main
    ```

    将上述脚本保存为 `completion_script.sh`，然后通过运行 `chmod +x completion_script.sh` 使其可执行。接下来，你可以在 Zsh 中使用 `completion_script.sh` 来实现自动补全功能。例如，在输入命令时，按下 Tab 键，脚本会显示相关的补全建议。
Get service status: 获取服务状态
Help about any command: 关于任何命令的帮助信息
? |-
    Help provides help for any command in the application.
    Simply type localaistack help [path to command] for full details.
: "“Help”功能可以为应用程序中的任何命令提供帮助。  \n只需输入 `localaistack help [命令路径]` 即可获取详细信息。"
Host to bind llama.cpp server: 需要绑定 llama.cpp 服务器的主机
Initialize LocalAIStack interactive configuration: 初始化 LocalAIStack 的交互式配置
Install a module: 安装一个模块。
'Installing module: %s': 正在安装模块：%s
JSON object passed to llama.cpp --chat-template-kwargs (e.g. '{"enable_thinking":false}'): '传递给 llama.cpp 的 JSON 对象（例如：`{"enable_thinking": false}`）'
List all available modules: 列出所有可用的模块。
List available LLM providers: 以下是可用的大型语言模型（LLM）提供商列表：
List downloaded models: 已下载的模型列表
LocalAIStack - Local AI workstation management: LocalAIStack – 本地人工智能工作站管理
? LocalAIStack is an open, modular software stack for building and operating local AI workstations. It provides unified control over AI development environments, inference runtimes, models, and applications.
: LocalAIStack 是一个开源的、模块化的软件栈，用于构建和运行本地人工智能工作站。它提供了对人工智能开发环境、推理运行时、模型以及应用程序的统一管理功能。
Manage AI models: 管理人工智能模型
Manage LLM providers: 管理大型语言模型（LLM）提供商
Manage services: 管理服务
Manage software modules: 管理软件模块
Maximum number of results per source: 每个来源的最大结果数量
Micro batch size for llama.cpp (0 = auto): '`lama.cpp` 的微批次大小（0 表示自动选择）：'
Min-p sampling for llama.cpp (0-1): '`lama.cpp` 中的 Min-p 采样（范围：0-1）'
Module %s installed successfully.: 模块 %s 安装成功。
Module %s purged successfully.: 模块 %s 已成功清除。
Module %s uninstalled successfully.: 模块 %s 已成功卸载。
'Module install failed: %s': 模块安装失败：%s
Port to bind llama.cpp server: 用于将代码绑定到 llama.cpp 服务器的端口。
Preferred interaction language: 首选的交互语言
Presence penalty for llama.cpp: lama.cpp 文件中存在的“惩罚机制”（即某些代码或设计导致的负面影响）
Purge a module: “Purge a module” 的意思是“删除或清除某个模块”。
'Purging module: %s': 正在清理模块：%s
Remove a downloaded model: 删除已下载的模型
Repeat penalty for llama.cpp: '`lama.cpp` 文件中的重复代码惩罚机制'
Run a local model: 运行一个本地模型。
Run module-specific settings: 运行模块特定的设置
Sampling temperature for llama.cpp: '`lama.cpp` 文件中的采样温度'
Search for models: 搜索模型
Show system information: 显示系统信息
SiliconFlow API key: SiliconFlow API密钥
Source of the model (ollama, huggingface, modelscope): 模型的来源（ollama、huggingface、modelscope）
Source to download from (ollama, huggingface, modelscope): 可以从以下来源下载模型：ollama、huggingface、modelscope。
Source to search (ollama, huggingface, modelscope, or all): 要搜索的来源（ollama、huggingface、modelscope 或所有来源）：
Specific GGUF filename to run: 要运行的具体 GGUF 文件名
Specific model file to download (e.g. Q4_K_M.gguf): 需要下载的具体模型文件（例如：Q4_K_M.gguf）
Start a service: 启动一个服务
Stop a service: 停止一个服务
System management: 系统管理
Tensor split for multi-GPU (comma-separated percentages): 用于多 GPU 的张量分割（百分比值以逗号分隔）
Top-k sampling for llama.cpp (0 disables): 对于 llama.cpp，采用前 k 个样本的采样方法（k=0 时禁用该功能）。
Top-p nucleus sampling for llama.cpp (0-1): 对于 llama.cpp 文件，采用“Top-p 核心采样”方法（范围为 0-1）
Translation API base URL: 翻译 API 的基础 URL
Translation model: 翻译模型
Translation provider: 翻译服务提供商
Translation timeout in seconds: 翻译超时时间（以秒为单位）
Uninstall a module: 卸载一个模块
'Uninstalling module: %s': 正在卸载模块：%s
Update a module: 更新一个模块
config file (default is $HOME/.localaistack/config.yaml): 配置文件（默认位于 $HOME/.localaistack/config.yaml）
config file path (default is ~/.localaistack/config.yaml): 配置文件路径（默认为 ~/.localaistack/config.yaml）
disable completion descriptions: 禁用补全描述（Disable completion descriptions）
help for download: 关于下载的帮助信息
help for install: 安装帮助
help for list: 关于列表的帮助信息
help for purge: "关于“purge”的帮助：  \n“Purge”是一个常见的计算机操作术语，通常指的是清除或删除某些数据、文件或系统资源。具体含义取决于上下文：  \n\n1. **数据清理**：在数据库或文件系统中，可以清除过时、重复或不需要的数据。例如，删除旧的日志文件或备份文件。  \n2. **系统优化**：在操作系统或应用程序中，可以清除临时文件、缓存或无用的进程，以释放系统资源并提高性能。  \n3. **安全策略**：在网络安全领域，可以清除恶意软件或入侵痕迹，以保护系统安全。  \n\n例如：  \n- 在数据库中，可以使用`DELETE`语句来清除旧记录：  \n  ```sql\n  DELETE FROM table_name WHERE creation_date > '2021-01-01';\n  ```  \n- 在操作系统上，可以使用磁盘清理工具来清理临时文件：  \n  ```bash\n  cleanmgr /s\n  ```  \n\n如果您需要更具体的帮助或指导，请提供更多关于您所遇到的问题的详细信息。"
help for run: 关于如何运行的帮助
help for search: 搜索帮助
help for uninstall: 关于卸载的帮助：
'install step %s failed: %w': 安装步骤 %s 失败：%w
vLLM GPU memory utilization (0-1, safetensors only): vLLM GPU内存利用率（0-1，仅包括安全传感器数据）
vLLM max model length (safetensors only): vLLM的最大模型长度（仅限使用安全张量（safetensors））
verbose output: 详细输出（verbose output）
详细输出（verbose output）: 详细输出（verbose output）：
详细输出（verbose output）：: 详细输出（verbose output）：
配置文件（默认位于 $HOME/.localaistack/config.yaml）: 配置文件（默认位于 $HOME/.localaistack/config.yaml）
