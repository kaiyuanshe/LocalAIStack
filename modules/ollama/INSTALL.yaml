apiVersion: las.installspec/v0.1.2
kind: InstallPlan

id: ollama
category: runtime

supported_platforms:
  - ubuntu-22.04
  - ubuntu-24.04

install_modes:
  - native
  - container

rebuild_modes:
  - none
  - soft
  - full

tools_required:
  - bash
  - service_manager

description:
  purpose: >
    Install and manage Ollama as a local LLM inference runtime on a single node.
  scope:
    - single-node inference
    - local-only API access
    - developer and research usage
  non_goals:
    - distributed inference
    - model training
    - multi-user access control

dependencies:
  system:
    - curl
    - ca-certificates
    - systemd
  modules: []
  capabilities: []
  optional:
    gpu:
      - cuda-runtime

preconditions:
  - id: P10
    intent: Verify OS version
    tool: bash
    command: lsb_release -rs
    expected:
      one_of: ["22.04", "24.04"]
    idempotent: true

  - id: P20
    intent: Ensure curl is available
    tool: bash
    command: command -v curl >/dev/null
    expected:
      exit_code: 0
    idempotent: true

  - id: P30
    intent: Check port availability (11434)
    tool: bash
    command: ss -ltn | grep -E ':(11434)\b' || true
    expected:
      exit_code: 0
    idempotent: true

decision_matrix:
  default: native
  rules:
    - when:
        shared_environment: true
      use: container
    - when:
        prefer_isolation: true
      use: container

environment_rebuild:
  detect:
    - id: D10
      intent: Detect existing ollama binary
      tool: bash
      command: which ollama || true
      expected:
        exit_code: 0
      idempotent: true

    - id: D20
      intent: Detect systemd service unit
      tool: bash
      command: systemctl status ollama >/dev/null 2>&1 && echo "present" || true
      expected:
        exit_code: 0
      idempotent: true

  soft_cleanup:
    - id: C10
      intent: Soft cleanup (remove known conflicting installations, preserve data)
      tool: bash
      command: sudo bash scripts/cleanup_soft.sh
      expected:
        exit_code: 0
      idempotent: true

  full_cleanup:
    - id: C20
      intent: Full cleanup (destructive rebuild)
      tool: bash
      command: sudo bash scripts/cleanup_full.sh
      expected:
        exit_code: 0
      idempotent: true

install:
  native:
    - id: S10
      intent: Download official installer script
      tool: bash
      command: curl -fsSL https://ollama.com/install.sh -o /tmp/ollama-install.sh
      expected:
        file_exists: /tmp/ollama-install.sh
      idempotent: true

    - id: S20
      intent: Run installer
      tool: bash
      command: sudo bash /tmp/ollama-install.sh
      expected:
        exit_code: 0
      on_fail:
        next:
          - T10
      idempotent: true

    - id: S30
      intent: Start ollama service
      tool: service_manager
      command:
        action: start
        service: ollama
      expected:
        status: running
      idempotent: true

  container:
    - id: S100
      intent: Container mode is not implemented in v0.1.2 reference module
      tool: bash
      command: |
        echo "Container mode not implemented for this reference module yet."
        exit 2
      expected:
        exit_code: 2
      idempotent: true

# Optional troubleshooting actions referenced by on_fail (kept machine-readable)
troubleshooting:
  - id: T10
    intent: Collect service logs for diagnosis
    tool: bash
    command: |
      set -e
      systemctl status ollama || true
      journalctl -u ollama -n 120 --no-pager || true
    expected:
      exit_code: 0
    idempotent: true

configuration:
  required: false
  defaults:
    bind: 127.0.0.1
    port: 11434
    data_dir: ~/.ollama
  templates: []

verification:
  script: scripts/verify.sh

rollback:
  script: scripts/rollback.sh

uninstall:
  script: scripts/uninstall.sh
  preserves:
    - ~/.ollama

purge:
  script: scripts/purge.sh
  destructive: true

security:
  network:
    bind: localhost
    auth: none
  privileges:
    requires_sudo: true
  notes:
    - Ollama listens on localhost by default.
    - Do not expose port 11434 publicly without access controls.

references:
  - https://ollama.com
  - https://github.com/ollama/ollama
